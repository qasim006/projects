{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "376e5444",
      "metadata": {
        "id": "376e5444"
      },
      "source": [
        "## Automatic Text Summarization Using Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d966f2",
      "metadata": {
        "id": "e3d966f2"
      },
      "source": [
        "##### Importing Necessary Libraries\n",
        "> <em>bs4</em> or Beautiful Soup is a parser for HTML and XML<br>\n",
        "> <em>urllib.request</em> is an extensible library for opening URLs<br>\n",
        "> <em>re</em> is a regex tool for python<br>\n",
        "> <em>nltk</em> is a suite of libraries and programs for NLP using Python<br>\n",
        "> <em>operator</em> is a module for efficient implementations of common operations <br>\n",
        "> <em>nltk.download</em> retrieves data from the nltk library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51bfe018",
      "metadata": {
        "id": "51bfe018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5af39bc-77b6-49ca-c158-7daa625b6465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import bs4 as bs\n",
        "import urllib.request as url\n",
        "import re\n",
        "import nltk\n",
        "import operator\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145e3de3",
      "metadata": {
        "id": "145e3de3"
      },
      "source": [
        "##### Getting Data\n",
        "><p>The data stores the read content from the url. The website HTML is parsed, and the text enclosed in <em>p</em> tags make up the article paragraphs. The text used for summarization is a string of those paragraphs</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1003a7f6",
      "metadata": {
        "id": "1003a7f6"
      },
      "outputs": [],
      "source": [
        "# https://en.wikipedia.org/wiki/AI\n",
        "# https://en.wikipedia.org/wiki/Russo-Ukrainian_War\n",
        "# https://en.wikipedia.org/wiki/Uno_(card_game)\n",
        "\n",
        "data = url.urlopen('https://en.wikipedia.org/wiki/AI').read()\n",
        "\n",
        "parsed = bs.BeautifulSoup(data,'html.parser')\n",
        "\n",
        "paragraphs = parsed.find_all('p') \n",
        "\n",
        "raw_text = \"\"\n",
        "for paragraph in paragraphs: raw_text += paragraph.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4803f6",
      "metadata": {
        "id": "eb4803f6"
      },
      "source": [
        "##### Preprocessing/Cleaning the Text\n",
        "><p>Removing square brackets, special characters and digits, and the extra spaces in the text</p>\n",
        "><p>Raw text contains the original article's text, and the formatted text removes the punctuation and special charachters so that the word frequency table can be made</p>\n",
        "><p>The raw text is tokenized into sentences, and the formatted text is tokenized into words. Ignorable stopwords are also stored. We initiliaze the frequency and score dictionaries as well</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc8c681",
      "metadata": {
        "id": "4fc8c681"
      },
      "outputs": [],
      "source": [
        "raw_text = re.sub(r'\\[.*\\]', ' ', raw_text)\n",
        "raw_text = re.sub(r'\\s+', ' ', raw_text)\n",
        "sentences = nltk.sent_tokenize(raw_text)\n",
        "\n",
        "formatted_text = re.sub('[^a-zA-Z0-9]', ' ', raw_text )\n",
        "formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
        "words = nltk.word_tokenize(formatted_text)\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "frequencies = {}\n",
        "scores = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb17f71d",
      "metadata": {
        "id": "bb17f71d"
      },
      "source": [
        "##### Getting Word Frequency\n",
        ">We loop through the words and append the count of the word in a frequency table every time it is encountered\n",
        "<br><br>\n",
        ">Diving each table entry by the max frequency gives the weighted frequencies for each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c25e343",
      "metadata": {
        "id": "6c25e343"
      },
      "outputs": [],
      "source": [
        "for word in words:\n",
        "    if word not in stopwords:\n",
        "        if word not in frequencies.keys(): \n",
        "            frequencies[word] = 1\n",
        "        else: \n",
        "            frequencies[word] += 1\n",
        "\n",
        "max_frequency = max(frequencies.values())\n",
        "\n",
        "for word in frequencies.keys():\n",
        "    frequencies[word] /= max_frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa7ccdd",
      "metadata": {
        "id": "3fa7ccdd"
      },
      "source": [
        "##### Finding the Most Important Sentences\n",
        "> We loop through the sentences, and tokenize each one into words. If the word has a frequency, we look at the ones with sentences less than <em>n</em> words (30 in our case)\n",
        "<br><br>\n",
        "> The score for the sentence is the weighted frequency of the first word in the sentence, if it is a new entry. Otherwise the weighted frequency of each word is added to the sentence. So high frequency words in a sentence will cause a high score sentence, and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bc2b7a",
      "metadata": {
        "id": "f8bc2b7a"
      },
      "outputs": [],
      "source": [
        "for sentence in sentences:\n",
        "    sentence_words = nltk.word_tokenize(sentence.lower())\n",
        "    for word in sentence_words:\n",
        "        if word in frequencies.keys():\n",
        "            if len(sentence.split(' ')) < 30:\n",
        "                if sentence not in scores.keys():\n",
        "                    scores[sentence] = frequencies[word]\n",
        "                else:\n",
        "                    scores[sentence] += frequencies[word]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d9ab21",
      "metadata": {
        "id": "c9d9ab21"
      },
      "source": [
        "##### Summarizing\n",
        ">The scores are sorted in descending order. The loop then gets the top <em>n</em> sentences (3 or 5 in our case), joins the sentences with spaces, and then outputs them as a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fef5038",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "0fef5038",
        "outputId": "dbd1ad67-8f24-4c99-aedf-04cde3f2ed27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. AI founder John McCarthy said: \"Artificial intelligence is not, by definition, simulation of human intelligence\". Data analysis is a fundamental property of artificial intelligence that enables it to be used in every facet of life from search results to the way people buy product. By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\". '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "scores = dict( sorted(scores.items(), key=operator.itemgetter(1), reverse=True))\n",
        "count = 0\n",
        "summary_text = ''\n",
        "for sentence in scores.keys():\n",
        "    # 3 or 5 sentence summaries\n",
        "    if count <= 5: \n",
        "        summary_text += sentence + ' '\n",
        "        count +=1 ;\n",
        "\n",
        "summary_text"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Summarizer",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}